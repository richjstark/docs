{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the CLIMB-BIG-DATA documentation!</p> <p>These docs are designed to help you get the best out of the CLIMB-BIG-DATA infrastructure.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>How to access CLIMB-BIG-DATA and find your way around via. the Bryn web interface.</p> <p>Registration How to register and access CLIMB-BIG-DATA.</p> <p>Authentication How to login to Bryn, and setup two-factor authentication.</p>"},{"location":"#notebook-servers","title":"Notebook Servers","text":"<p>Everything you need to understand, use and get the most out of Jupyter Notebook Servers.</p> <p>Introduction An introduction to the what and why of notebook servers.</p> <p>Quick start How to launch, access and get started using a notebook server.</p> <p>Using the terminal How to use the terminal inside a notebook server, with an explanation of caveats.</p> <p>Understanding storage An explanation of the different storage options available and when to use what.</p> <p>Installing software with Conda How to install software using Conda, in the context of a containerized environment.</p> <p>Using Nextflow How to use Nextflow with CLIMB-BIG-DATA.</p> <p>Metagenomics walkthrough A simple walk-through of some CLIMB-BIG-DATA functionality</p>"},{"location":"getting-started/authentication/","title":"Authentication","text":""},{"location":"getting-started/authentication/#where-do-i-sign-in","title":"Where do I sign in?","text":"<p>All CLIMB-BIG-DATA resources are accessed via the Bryn web interface.</p> <p>You can login to Bryn at https://bryn.climb.ac.uk/account/login/</p>"},{"location":"getting-started/authentication/#two-factor-authentication","title":"Two-factor authentication","text":"<p>Two-factor authentication is mandatory, and users will be required to set this up on first login. This means that a code will be required from an authenticator app on a mobile or desktop device, in addition to your password.</p> <p>For convenience, if using the same browser and device, you will only be required to enter this every 30 days.</p>"},{"location":"getting-started/authentication/#initial-setup","title":"Initial setup","text":"<p>When you first login to your account, authentication will need to be set up. You will be presented with the following interface:</p> <p></p> <p>Hit enable and proceed to follow the instructions. You will be prompted to install an authenticator app on your mobile device. Our current recommendations (in order) are:</p> <ul> <li>Authy (Desktop app available)</li> <li>Microsoft Authenticator</li> <li>Google Authenticator</li> </ul> <p>Info</p> <p>Authy and Microsoft Authenticator make it easier to backup and recover your codes in the case of loss or change of device, whereas Google requires a manual export/import.</p> <p>Please do enable backups for your app at this stage, before you forget!</p>"},{"location":"getting-started/authentication/#authy-backups","title":"Authy backups","text":"<p>Authy has a backup feature to enable recovery in case you lose or replace your phone. Your data is encrypted and only decrypted on the devices using a password that only you will know.</p> <p>See the Authy documentation for further details.</p>"},{"location":"getting-started/authentication/#microsoft-authenticator-backups","title":"Microsoft Authenticator backups","text":"<p>See the Microsoft documentation for details on how to enable this.</p>"},{"location":"getting-started/authentication/#scanning-the-qr-code","title":"Scanning the QR code","text":"<p>In all authenticator apps, the default way to add an app is to scan a QR code. Bryn will present the code for you to scan, and ask you to enter the generated token before it expires (every 30s). If it does expire while you are typing, just enter the new one.</p> <p>Congratulations, two-factor authentication is now enabled.</p>"},{"location":"getting-started/authentication/#backup-codes","title":"Backup codes","text":"<p>You will now have the option to generate backup codes. This is a list of single-use codes that enable you to gain access in the event of the loss of your device. This is strongly recommended.</p> <p>Follow the link to 'setup your backup codes' as seen in the screenshot below:</p> <p></p> <p>Next, hit the 'Generate tokens' button.</p> <p></p> <p>Print, save or otherwise make a note of these codes and keep them in a secure place.</p> <p>You can now proceed to the Bryn dashboard</p>"},{"location":"getting-started/authentication/#logging-in-to-bryn-after-2fa-is-set-up","title":"Logging in to Bryn after 2FA is set up","text":"<p>When you next login to Bryn, after entering your username and password, a token will be requested from your 2FA device:</p> <p></p> <p>If you wish, mark the check box \"Don't ask again on this device for 30 days\"</p>"},{"location":"getting-started/authentication/#using-one-of-your-backup-tokens","title":"Using one of your backup tokens","text":"<p>If you don't have your device to hand, or you've lost it, you can use a single-use backup token.</p> <p>To use a backup token, follow the link here:</p> <p></p>"},{"location":"getting-started/authentication/#managing-two-factor-authentication","title":"Managing two-factor authentication","text":"<p>When logged in to the Bryn dashboard, navigate to 'User profile' under the 'Team management' menu.</p> <p></p> <p>Here, you can view or generate your backup codes and disable 2FA (for example you wish to use a different device or app).</p> <p></p>"},{"location":"getting-started/authentication/#disable-your-current-2fa-device-switch-device-or-app","title":"Disable your current 2FA device (switch device or app)","text":"<p>If you wish to change your device or app, you'll need to disable your current two-factor authentication. This will log you out and restart the process.</p> <p>Please note: if you are only changing device, the best option is to use the backup/recovery method for your authenticator app.</p> <p>If you are sure you'd like to start again, hit the 'Disable Two-Factor Authentication' button.</p> <p></p> <p>Confirm you are sure, and you will be logged out. Log back in to restart the 2FA setup process with your new app or device.</p>"},{"location":"getting-started/authentication/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"getting-started/authentication/#i-can-no-longer-access-my-2fa-device-or-phone-and-i-cant-recover-from-a-backup","title":"I can no longer access my 2FA device or phone, and I can't recover from a backup","text":"<p>If you are still logged in to bryn, follow the steps in 2FA Management.</p> <p>Otherwise, please contact our support team at support@climb.ac.uk</p>"},{"location":"getting-started/authentication/#why-are-my-authenticator-codes-not-working","title":"Why are my authenticator codes not working ?","text":"<p>If you have setup your authentication but the 6-digit code does not work, you may need to check that the time on your device is set to auto-sync. Without this setting, the time on the device and server will not match and the codes will not work. This may be a term such as: \"Use network provided time/time-zone\".</p>"},{"location":"getting-started/authentication/#why-are-my-backup-codes-not-working","title":"Why are my backup codes not working ?","text":"<p>You will only have X backup codes to use. Once they have been used up, you will not be able to access your account. These are to be used when either the app/desktop authenticators do not work. You can check your codes and how many you have left, please see 2FA Management.</p>"},{"location":"getting-started/authentication/#i-have-the-authenticator-app-why-wont-it-let-me-scan-anything","title":"I have the authenticator app, why wont it let me scan anything ?","text":"<p>If you are not prompted to scan the QR code, you may need to change the permissions for the app to access your camera.</p>"},{"location":"getting-started/authentication/#i-dont-have-a-smart-phone-how-do-i-login","title":"I don\u2019t have a smart phone, how do I login ?","text":"<p>There are desktop versions of authenticators that you can use, such as Authy.</p>"},{"location":"getting-started/how-to-register/","title":"How to register","text":""},{"location":"getting-started/how-to-register/#who-can-register","title":"Who can register","text":"<p>We divide users into three categories:</p> <ul> <li>Primary users: Those with salaried positions in UK academic institutions, government agencies or healthcare systems who have the status of independent researchers and/or team leaders.</li> <li>Secondary users: Those working under the direction of primary users who include students, post-doctoral researchers and overseas collaborators.</li> <li>Industrial users: Users in industry should contact us to discuss terms and conditions for industrial users.</li> </ul> <p>Primary users should head over to the Bryn registration page to get started.</p> <p>To add secondary users to a team, please see inviting users to your team</p> <p>Warning</p> <p>Please note: only primary users should register to create a new team</p>"},{"location":"getting-started/how-to-register/#primary-user-registration","title":"Primary user registration","text":"<p>When you head over to the Bryn registration page, you should see the registration form:</p> <p></p> <p>Accept the terms if you are happy, and you will then be asked information regarding your \u201cPrimary user details\u201d including contact information and your position. You will also be asked about your \u201cTeam details\u201d for your CLIMB-BIG-DATA team account. This includes information on where you currently work and why you would like to use CLIMBs resources.</p> <p>The registration request will be reviewed by a member of our management team. Please provide as much information as possible about your role and research to speed up the process. If we do not feel enough information has been provided, we may contact you. If your registration is successful, you will receive a verification email. Following verification, you will be taken to the Bryn portal.</p> <p>Warning</p> <p>Please we aware that primary users will be provided to renew a group license every 3 months. For paying users, the length of a valid license period will be extended. If this license is not renewed, your access to resources will be blocked. You will be sent an email reminder for this.</p>"},{"location":"notebook-servers/","title":"Notebook Servers","text":"<p>Everything you need to understand, use and get the most out of Jupyter Notebook Servers.</p> <p>Introduction An introduction to the what and why of notebook servers.</p> <p>Quick start How to launch, access and get started using a notebook server.</p> <p>Using the terminal How to use the terminal inside a notebook server, with an explanation of caveats.</p> <p>Understanding storage An explanation of the different storage options available and when to use what.</p> <p>Installing software with Conda How to install software using Conda, in the context of a containerized environment.</p> <p>Using Nextflow How to use Nextflow with CLIMB-BIG-DATA.</p>"},{"location":"notebook-servers/installing-software-with-conda/","title":"Installing software with Conda","text":"<p>Its critical to understand how conda works with notebook servers, and the caveats vs using it locally or on a VM.</p> <p>Firstly, understand that you cannot install new software to the base conda environment. Lets have a look at why.</p> <pre><code>$ conda info --envs\nbase                     /opt/conda\n</code></pre> <p>The base conda env is installed at <code>/opt/conda</code>. Since we are running instide a container, any changes made to this part of the filesystem will not be retained once the container is stopped and restarted (unlike your home dir and shares, which are persisted).</p> <p>We've made the base environment read only to prevent any confustion.</p> <p>Tip</p> <p>You must create a new conda environment before installing software!</p>"},{"location":"notebook-servers/installing-software-with-conda/#default-condarc","title":"Default .condarc","text":"<p>When you first launch a notebook server, we generate a default <code>.condarc</code> in your home directory. This sets the path for your new environments to <code>/shared/team/conda/$JUPYTER_USERNAME</code>. Why? As mentioned above, your home directory is relatively small vs your team share, so it makes sense to use the larger mount. In additon, it becomes easy to share conda environments with other team members.</p> <pre><code>jovyan:~$ cat ~/.condarc\nenvs_dirs:\n  - /shared/team/conda/demouser.andy-bryn-dev-t\n[...]\n</code></pre>"},{"location":"notebook-servers/installing-software-with-conda/#creating-new-conda-environments","title":"Creating new conda environments","text":"<p>Understanding the above, you can create new conda environments in the usual way. The only caveat is that if you wish to use this environment with ipython notebooks, you must install <code>ipykernel</code>.</p> <p>Warning</p> <p>If you don't install <code>ipykernel</code> in a new conda environment, it won't show up on the launcher or be available to select within the python notebooks interface. However, you can use the environment just fine within a terminal.</p> <p>Lets go ahead and install bactopia as an example.</p> <p>If you try listing channels, you'll see you already have conda-forge and bioconda set.</p> <pre><code>jovyan:~$ conda create -y -n bactopia bactopia ipykernel\n...grab a coffee...\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate bactopia\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> <p>Done. Listing your envs again to confirm the location:</p> <pre><code>jovyan:~$ conda info --envs\n# conda environments:\n#\nbase                     /opt/conda\nbactopia                 /shared/team/conda/demouser.andy-bryn-dev-t/bactopia\n</code></pre> <p>And finally lets activate the environment</p> <pre><code>jovyan:~$ conda activate bactopia\n(bactopia) jovyan:~$ bactopia --version\nbactopia 2.2.0\n</code></pre>"},{"location":"notebook-servers/introduction/","title":"Introduction to Jupyter Notebook Servers","text":""},{"location":"notebook-servers/introduction/#what-is-a-notebook-server","title":"What is a notebook server?","text":"<p>CLIMB hosts a bespoke installation of JupyterHub, which allows users to interact with our powerful compute infrastructure through a web interface. Rather than creating Virtual Machines and accessing them via SSH, users in a team can each spin up their own 'notebook server', and access it directly via a URL.</p> <p>But what actually is a notebook server? In the CLIMB environment, notebook servers are container instances that run inside a pod on a Kubernetes cluster. Don't worry too much about that though. You can think of them as your own server. Just like a VM, you'll have terminal access and a filesystem. Your home directory is persisted, even when your server is terminated, and you'll have out-of-the-box access to things like a shared team volume, s3 buckets and more.</p>"},{"location":"notebook-servers/introduction/#advantages-of-containers-and-notebook-servers","title":"Advantages of containers and notebook servers","text":"<p>So, why would you want to use these over Virtual Machines?</p> <ul> <li>Ease of setup: rather than being faced with a blank base OS with nothing installed, you'll find that the base CLIMB-BIG-DATA container image comes pre-configured and ready to use. Conda, nextflow, and a variety of command line tools are pre-installed and configured to work with your other team resources.</li> <li>Flexible resource usage: depending on your tier, you'll have access to a minimum and maximum amount of resource for your notebook servers (vCPUs and memory) for each team member. In addition, you can access huge additional cluster resource via. nextflow running against our Kubernetes (K8s) execution environment. If you prefer larger notebook servers to run things locally, you can upgrade your tier and gain access immediately: no need to start again and re-install everything!</li> <li>Ease of access: No more losing your keys and getting locked out of your VM. All you need is Bryn access, and you can always access your notebook servers via our simple web interface. You can even create time-limited sharing links that don't require a password login!</li> <li>Affordable GPU access: Under the VM model, GPUs were an expensive resource because they were 'locked up' in individual VMs until terminated. Using containers, we can share GPU access more equitably, making them more affordable. Not only that, but the CLIMB-BIG-DATA base image is pre-configured with the necessary tools and drivers needed to use our powerful A100s out of the box.</li> <li>Easier collaboration and sharing: The virtual machine usage model tended to produce siloed data on volumes that could only be attached to one VM at a time. The collection of software installed on a VM was also siloed, in the sense that it was not easily reproduced from scratch. Under the new usage model of container notebooks, you'll find a a pre-mounted share at <code>/shared/team</code> that every team member has read/write access to, and backed by our high speed SSDs. Your S3 bucket keys are also pre-injected as environment variables, making bucket access a breeze. Need to share something with an external collaborator? Just make a public bucket and upload your files.</li> <li>The ideal teaching environment: Running workshops becomes a breeze. Simply create a team on Bryn, invite attendees and prepare your materials on the shared team drive or S3 buckets. No more losing half a day getting everyone logged in via SSH.</li> </ul>"},{"location":"notebook-servers/introduction/#is-this-just-for-beginners","title":"Is this just for beginners?","text":"<p>Absolutely not. Whilst the new service is certainly far easier to get started with, it is also far more powerful for advanced users. We've ensured that each team gets a pre-mounted kubernetes service user with permissions scoped to your team's namespace. This allows users to run nextflow workflows on the external K8s execution environment, but also to run other containers in pods within their namespace via kubectl.</p>"},{"location":"notebook-servers/introduction/#how-do-i-get-started","title":"How do I get started?","text":"<p>Head over to our quick start guide!</p>"},{"location":"notebook-servers/quick-start/","title":"Notebook servers, quick start","text":""},{"location":"notebook-servers/quick-start/#how-to-launch-and-access-a-notebook-server","title":"How to launch and access a notebook server","text":"<ol> <li>First, log in to Bryn</li> <li>Using the navigation menu on the left hand side, select 'Notebook servers' under the 'Compute' subheading</li> <li>Click the 'Launch notebook server' green action button on the right hand side</li> <li>Select a profile, for example 'Standard server' or 'GPU server' (tier dependent)</li> <li>Click 'Launch Server' and monitor the progress bar</li> <li>Once ready, click the url beneath the 'User notebook server'</li> <li>On first login, you may be asked to authorize access to your Bryn account. Click 'Authorize'</li> <li>The JupyterLab interactive computing interface should open in a new tab.</li> </ol>"},{"location":"notebook-servers/quick-start/#finding-your-way-around-the-jupyterlab-interface","title":"Finding your way around the JupyterLab interface","text":"<p>We highly recommend spending a few moments familiarizing yourself with the basic JupyterLab interface. Head on over to the JupyterLab interface docs to get started.</p> <p>Fundamentally, your screen is divided into a few areas. You'll see context menus at the top (File, Edit, View, Run etc.), a file browser pane on the left, and an activity area that initially displays a launcher interface with tiles. Clicking on one of these tiles will open a new tab in the activity area.</p>"},{"location":"notebook-servers/quick-start/#what-now","title":"What now?","text":"<p>Since most users are familiar with using a terminal to access a system shell (just like on a VM!), lets start there.</p>"},{"location":"notebook-servers/understanding-storage/","title":"Understanding storage","text":""},{"location":"notebook-servers/understanding-storage/#home-directory","title":"Home directory","text":"<p>Your home directory <code>/home/jovyan</code> (~) is actually a mount that maps to persistent storage for your CLIMB user. Anything written here will be persisted, even after the container is stopped and restarted. Home directories are intentionally small (20GB by default, depending on your tier) and are not intended for large conda environments or databases, for example. This will make more sense once you understand the other storage options available to you.</p> <p>Your home directory is also the default/base location for the file browser pane on the left.</p>"},{"location":"notebook-servers/understanding-storage/#team-share","title":"Team share","text":"<p>You'll find a writable 'team share' mounted at <code>/shared/team</code>, and symlinked to your home directory by default as <code>shared-team</code> (hence it shows in the file browser).</p> <p>This is a larger storage location (1TB+ depending on tier) that all team members share simultaneous read/write access to. It's also SSD-backed, so its extremely fast.</p>"},{"location":"notebook-servers/understanding-storage/#s3-buckets","title":"S3 Buckets","text":"<p>If you've started using S3 buckets via Bryn, your access keys will have already been injected into the notebook server as environment variables. We've also pre-configured <code>aws cli</code> and <code>s3cmd</code> to use the CLIMB S3 endpoints by default. As a result, you should be able to access buckets with no additional setup.</p>"},{"location":"notebook-servers/understanding-storage/#read-only-shares","title":"Read-only shares","text":"<p>In addition to the team share, you may also notice additional mounts under <code>/shared/</code>, including at least <code>/shared/public</code>. Here you will find read-only data and resources provided by CLIMB, that may be useful to microbial bioinformatics workflows. Initially we have populated these shares with a few key resources:</p> <ul> <li>Ben Langmead's Kraken2/Bracken Refseq Databases - in <code>/shared/public/db/kraken2</code></li> <li>The NCBI GenBank non-redundant protein BLAST database - in <code>/shared/public/db/blast</code></li> </ul>"},{"location":"notebook-servers/using-nextflow/","title":"Using Nextflow","text":""},{"location":"notebook-servers/using-nextflow/#what-is-nextflow-and-how-does-it-fit-into-the-climb-big-data-platform","title":"What is nextflow and how does it fit into the CLIMB-BIG-DATA platform?","text":"<p>In their words, \"Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.\"</p> <p>We've made Nextflow a first class citizen in CLIMB-BIG-DATA. You'll find an up-to-date stable version pre-installed in your notebook server and, more importantly, pre-configured to take advantage of our scalable kubernetes infrastructure.</p>"},{"location":"notebook-servers/using-nextflow/#how-can-i-start-using-nextflow","title":"How can I start using nextflow?","text":"<p>Create a notebook server and start a terminal session. Now type:</p> <pre><code>jovyan:~$ nextflow -v\n[...]\nnextflow version 23.04.0.5857\n</code></pre> <p>Your version may be newer than the above, but the point is that nextflow is pre-installed and you should usually use this binary rather than downloading another. You don't need to follow the installation instructions in the nextflow docs.</p>"},{"location":"notebook-servers/using-nextflow/#tutorial-using-nf-core","title":"Tutorial using nf-core","text":"<p>nf-core is a community-curated set of analysis pipelines that use nextflow. We'll try running nf-core/rnaseq as an example, to demonstrate some features of how nextflow is configured to work on CLIMB.</p> <p>We'll be using <code>-profile test</code>, which includes links to appropriate data. As a result, we'll only need to specify <code>--outdir</code> for now.</p> <pre><code>jovyan:~$ nextflow run nf-core/rnaseq -profile test --outdir nfout\n[...]\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:BAM_SORT_STATS_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_IDXSTATS -\n[c5/3af707] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_QUANT (RAP1_UNINDUCED_REP1)                 [ 50%] 1 of 2\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_TX2GENE                                     -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_TXIMPORT                                    -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE                                     -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE_LENGTH_SCALED                       -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_GENE_SCALED                              -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTIFY_STAR_SALMON:SALMON_SE_TRANSCRIPT                               -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:DESEQ2_QC_STAR_SALMON                                                   -\n[9f/b3b437] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MARKDUPLICATES_PICARD:PICARD_MARKDUPLICATES (RAP1_UNINDUCED_REP2)   [  0%] 0 of 2\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MARKDUPLICATES_PICARD:SAMTOOLS_INDEX\n[...etc...]\n</code></pre>"},{"location":"notebook-servers/using-nextflow/#what-is-going-on","title":"What is going on?","text":"<p>The pipeline is executing interdependent process in parallel, using the Kubernetes executor. That is to say, rather than running inside your notebook container directly, new Kubernetes pods are being created on the fly and spinning up containers for each process.</p> <p>Open another terminal tab alongside, while the workflow is still executing, and try:</p> <pre><code>jovyan:~$ kubectl get pods\nNAME                                         READY   STATUS              RESTARTS   AGE\njupyter-demouser-2eclimb-2dbig-2ddata-2dd   1/1     Running             0          12m\nnf-0e32425fc6d3dd42c9a3cbb8dd3ccc8c          0/1     Pending             0          4s\nnf-6171723bfd88a03f1417e2aead99f180          0/1     Terminating         0          12s\nnf-0e69c114a366b717ad115431277c01d7          1/1     Running             0          9s\nnf-31f1f1f534f51863f2e19320ca7447e0          0/1     ContainerCreating   0          4s\nnf-75dc1d907794e300ff2117a49be85c63          0/1     Pending             0          4s\nnf-8491621dd73c44811c49bee448771ae5          0/1     Completed           0          13s\nnf-9aad711fc9476c27e510b9402e3089d5          0/1     ContainerCreating   0          3s\nnf-12656c698dd83e7dc2a31e3c88818227          0/1     ContainerCreating   0          4s\nnf-a57472aaef0073c9e77a0a7e6001a849          0/1     ContainerCreating   0          3s\nnf-f8d94c52e9120b7b8ba117d530f77c3a          0/1     Completed\n</code></pre> <p>kubectl is the kubernetes command line tool. It's also pre-installed in the CLIMB notebook environment, and pre-configured with credentials that map to a ServiceUser for your team.</p> <p>This service user has certain privileges within your namespace, or in other words an isolated part of our cluster created specifically for your team. The command <code>kubectl get pods</code> above is returning a list of pods currently running in your namespace. You'll see your notebook server (<code>jupyter-demouser-2eclimb-2dbig-2ddata-2dd</code>) and a number of nextflow pods that are running workflow process containers. These will be in various states as they execute and then disappear.</p> <p>Once the workflow has finished, run the above command again:</p> <pre><code>jovyan:~$ kubectl get pods\nNAME                                         READY   STATUS    RESTARTS   AGE\njupyter-demouser-2eclimb-2dbig-2ddata-2dd   1/1     Running   0          17m\njovyan:~$\n</code></pre> <p>... and you're back to just your notebook server (you may also see those belonging to others in your Bryn team).</p>"},{"location":"notebook-servers/using-nextflow/#where-did-my-output-data-go","title":"Where did my output data go?","text":"<p>Once the workflow completes you'll see something like:</p> <pre><code>-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)- -[nf-core/rnaseq] Please check MultiQC report: 1/5 samples failed strandedness check.-\nCompleted at: 16-Apr-2023 13:26:51\nDuration : 5m 50s\nCPU hours : 0.4\nSucceeded : 196\n</code></pre> <p>We specified <code>nfout</code> as out outdir and you'll see the directory in the file browser on the left hand side of the JupyterLab interface. Take a look in <code>nfout/pipeline_info/</code> inside your file browser. Try double clicking on the various html, yaml and csv files here and you'll see that they open in new tabs for immediate reading.</p> <p>One thing to note here, when opening <code>html</code> files such as <code>execution_report_[date].html</code>, javascript is disabled in the tab by default. Right click the file and select <code>Open in New Browser Tab</code> from the context window to see the full report.</p>"},{"location":"notebook-servers/using-nextflow/#where-are-nextflow-assets-and-temporaryintermediate-workdir-outputs-stored","title":"Where are nextflow assets and temporary/intermediate (workdir) outputs stored?","text":"<p>By default, the CLIMB nextflow config sets nextflow 'home' to <code>/shared/team/nxf_work/$JUPYTERHUB_USER</code>. You'll see a number of subdirectories exist at that location, including <code>assets</code>, which will now contain the rnaseq workflow we just used, and <code>work</code>: where the intermediate outputs are located.</p> <pre><code>jovyan:~$ ls /shared/team/nxf_work/demouser.climb-big-data-d/\nassets  capsule  framework  plugins  secrets  tmp  work\n</code></pre>"},{"location":"notebook-servers/using-nextflow/#climb-nextflow-config-defaults","title":"CLIMB nextflow config defaults","text":"<p>We have tried to make it as easy as possible to use nextflow on CLIMB, and to make full use of available resources via our Kubernetes infrastructure. Out-of-the box, we set a number of configuration defaults.</p> <ul> <li>Nextflow home is set to <code>/shared/team/nxf_work/$JUPYTERHUB_USER</code></li> <li>WorkDir is set to <code>/shared/team/nxf_work/$JUPYTERHUB_USER/work</code></li> <li>Executor is set to <code>k8s</code> (plus some supporting config)</li> <li><code>/shared/team</code> and <code>/shared/public</code> (read only) are mounted as PVCs to all nextflow pods</li> <li>A K8s ServiceUser is pre-mounted (no credentials setup required)</li> <li>S3 bucket path-style access is enabled, with <code>s3.climb.ac.uk</code> set as the endpoint</li> <li>S3 keys have also been injected from Bryn</li> </ul>"},{"location":"notebook-servers/using-the-terminal/","title":"Jump into the terminal","text":"<p>Select <code>File -&gt; New -&gt; Terminal</code>, or click the Terminal icon on the launcher pane. You'll get a new terminal tab in the activity bar, and find yourself in a bash shell.</p> <p>Who is jovyan?</p> <p>Looking at your bash prompt, you'll notice that your username is <code>jovyan</code> (there's a backstory, but it means 'related to Jupyter'). Why is everyone's username the same? Your notebook server is running as a container. The container instance is private and linked to your Bryn user's storage, but the image it runs is the same for everyone. As a result, it is not necessary or desirable to have unique system users.</p> <p>TLDR: don't worry about it. Inside your notebook server, your username is <code>jovyan</code></p>"},{"location":"notebook-servers/using-the-terminal/#where-am-i-who-am-i","title":"Where am I? Who am I?","text":"<p>By default, you're in a bash shell running against the base operating system of the climb-jupyterhub container image (which is based on Ubuntu). You'll see in your bash prompt that you're in your home directory (represented by the tilde character <code>~</code>).</p> <pre><code>jovyan:~$ pwd\n/home/jovyan\n</code></pre> <p>What about sudo?</p> <p>jovyan doesn't have sudo privileges. This may seem restrictive, but we've pre-configured the climb-jupyter base image with everything you'd likely need sudo for pre-installed. Everything else should be installable via package managers, such as conda. You'll also be able to run Nextflow against out K8s execution environment 'out-of-the-box'.</p>"},{"location":"notebook-servers/using-the-terminal/#how-do-i-install-software","title":"How do I install software?","text":"<p>In the first instance, check out installing software with conda.</p>"},{"location":"walkthroughs/metagenomics-tutorial/","title":"CLIMB-BIG-DATA: Metagenomics in Brum","text":"<p>In this tutorial we will look at some metagenomics sequences that were sequenced from DNA extracted from the Worcester and Birmingham Canal at the University of Birmingham. In this case the canal water was fetched, DNA extracted and data was generated on the Oxford Nanopore MinION sequencing platform by Josh Quick.</p> <p></p> <p>The Worcester and Birmingham Canal near University Station (photo by Philip Halling)</p>"},{"location":"walkthroughs/metagenomics-tutorial/#before-you-begin","title":"Before you begin","text":"<p>This tutorial assumes you are working on CLIMB-BIG-DATA in a JupyterHub notebook. That is why many of the commands are prepended with the \"!\" (exclamation mark) symbol - which tells the notebook to run the command as if it was on the command line.</p> <p>You could do the tutorial without JupyterHub notebooks in the Terminal, in which case remove the \"!\" before each command.</p> <p>Have a look around this documentation site if you would like to understand more about what a JupyterHub notebook is.</p>"},{"location":"walkthroughs/metagenomics-tutorial/#setting-up-the-environment","title":"Setting up the environment","text":"<p>When working on CLIMB-BIG-DATA with Conda, always work in a new Conda environment. You will not be able to add software to the base environment.</p> <p>We speed the process along by using <code>mamba</code>, a drop-in replacement for <code>conda</code> to create a new environment.</p> <p>Our environment will be called <code>metagenomics-tutorial</code>.</p> <p>In order for this tutorial to function correctly within JupyterHub, also install <code>ipykernel</code>.</p> <pre><code>!mamba create -y -n metagenomics-tutorial ipykernel\n</code></pre> <p>Next, we will need the following software:</p> <ul> <li><code>kraken2</code> - a taxonomic profiler for metagenomics data</li> <li><code>krona</code> - an interactive visualiser for the output of Kraken2</li> <li><code>krakentools</code> - some useful scripts for manipulating Kraken2 output</li> <li><code>taxpasta</code> - a useful tool for converting and merging Kraken2 outputs into other formats like Excel</li> </ul> <pre><code>!mamba install -y kraken2 krona blast krakentools \n</code></pre> <p>Krona reminds us to run <code>ktUpdateTaxonomy.sh</code> before it can be used.</p> <pre><code>!ktUpdateTaxonomy.sh\n</code></pre> <p>Let's grab the CanalSeq data.</p> <p>This is available from the following link.</p> <p>Note that this link is served via the CLIMB S3 service. You can serve your own public data the same way simply by creating a public S3 bucket and uploading files to it!</p>"},{"location":"walkthroughs/metagenomics-tutorial/#running-kraken2","title":"Running Kraken2","text":"<p>You can test <code>kraken2</code> was installed correctly by running it with no command-line options.</p> <pre><code>!kraken2\n</code></pre> <p>Pre-computed Kraken2 databases are available on the <code>/shared/public</code> file system within CLIMB-BIG-DATA. These databases are downloaded from Ben Langmad's publicly available Kraken2 indexes page. These datasets are updated monthly and we will keep the latest versions available.</p> <p>The <code>/shared/public</code> area is designed to store frequently used, important databases for the microbial genomics community. We are just getting started building this resource so please contact us with suggestions for other databases you would like to see here.</p> <p>We can take a look at the databases that are available, and their sizes:</p> <pre><code>!du -h -d1 /shared/public/db/kraken2\n</code></pre> <pre><code>7.5G    /shared/public/db/kraken2/k2_pluspf_08gb\n9.1G    /shared/public/db/kraken2/k2_minusb\n556M    /shared/public/db/kraken2/k2_viral\n69G /shared/public/db/kraken2/k2_pluspf\n7.5G    /shared/public/db/kraken2/k2_standard_08gb\n15G /shared/public/db/kraken2/k2_pluspf_16gb\n65G /shared/public/db/kraken2/k2_standard\n15G /shared/public/db/kraken2/k2_standard_16gb\n264G    /shared/public/db/kraken2/downloads\n7.6G    /shared/public/db/kraken2/k2_pluspfp_08gb\n15G /shared/public/db/kraken2/k2_pluspfp_16gb\n145G    /shared/public/db/kraken2/k2_pluspfp\n619G    /shared/public/db/kraken2\n</code></pre> <p>We can run Kraken2 directly within this JupyterHub notebook which is running in a container. A standard container has 8 CPu cores and 64Gb of memory. Kraken2 doesn't run well unless the database fits into memory, so we can use one of the smaller databases for now such a <code>k2_standard_16gb</code> which contains archaea, bacteria, viral, plasmid, human and UniVec_Core sequences from RefSeq, but subsampled down to a 16Gb database. This will be fast, but we trade off specificity and sensitivity against bigger databases.</p> <pre><code>!kraken2 --threads 8 \\\n   --db /shared/public/db/kraken2/k2_standard_16gb \\\n   --output canalseq.hits.txt \\\n   --report canalseq.report.txt \\\n   canalseq.fasta\n</code></pre> <pre><code>Loading database information... done.\n37407 sequences (91.32 Mbp) processed in 4.876s (460.3 Kseq/m, 1123.76 Mbp/m).\n  12486 sequences classified (33.38%)\n  24921 sequences unclassified (66.62%)\n</code></pre> <p>About a third of sequences were classified and two-thirds were not.</p> <p>The <code>canalseq.report.txt</code> gives a human-readable output from Kraken2.</p> <pre><code>!cat canalseq.report.txt\n</code></pre> <p>People worry about getting Leptospirosis if they swim in the canal. Any evidence of Leptospira sp. in these results?</p> <pre><code>!grep Leptospira canalseq.report.txt\n</code></pre> <pre><code>  0.03  10  0   O   1643688           Leptospirales\n  0.03  10  0   F   170             Leptospiraceae\n  0.02  9   2   G   171               Leptospira\n  0.01  2   2   S   173                 Leptospira interrogans\n  0.00  1   1   S   28182                   Leptospira noguchii\n  0.00  1   1   S   28183                   Leptospira santarosai\n  0.00  1   1   S   1917830                 Leptospira kobayashii\n  0.00  1   1   S   2564040                 Leptospira tipperaryensis\n  0.00  1   0   G1  2633828                 unclassified Leptospira\n  0.00  1   1   S   1513297                   Leptospira sp. GIMC2001\n</code></pre>"},{"location":"walkthroughs/metagenomics-tutorial/#eek","title":"Eek!","text":"<p>It's easier to look at Kraken2 results visually using a Krona plot:</p> <pre><code>!ktImportTaxonomy -t 5 -m 3 canalseq.report.txt -o KronaReport.html\n</code></pre> <pre><code>   [ WARNING ]  Score column already in use; not reading scores.\nLoading taxonomy...\nImporting canalseq.report.txt...\nWriting KronaReport.html...\n</code></pre> <p>We can look at the Krona report directly within the browser by using the file navigator to the left - open up the KronaReport.html within the <code>shared-team</code> directory where we are working. Click around the Krona report to see what is in there.</p> <p>With the <code>extract_kraken_reads.py</code> script in <code>krakentools</code> we can quite easily extract a set of reads that we are interested in for further exploration: perhaps to use a more specific method like BLAST against a large protein database, or to extract for de novo assembly.</p> <pre><code>!extract_kraken_reads.py -k canalseq.hits.txt -s canalseq.fasta -r canalseq.report.txt -t 171 -o leptospira.fasta --include-children\n</code></pre> <pre><code>!extract_kraken_reads.py -k canalseq.hits.txt -s canalseq.fasta -r canalseq.report.txt -t 173 -o linterrogens.fasta --include-children\n</code></pre> <pre><code>PROGRAM START TIME: 04-16-2023 18:40:00\n&gt;&gt; STEP 0: PARSING REPORT FILE canalseq.report.txt\n    1 taxonomy IDs to parse\n&gt;&gt; STEP 1: PARSING KRAKEN FILE FOR READIDS canalseq.hits.txt\n    0.04 million reads processed\n    2 read IDs saved\n&gt;&gt; STEP 2: READING SEQUENCE FILES AND WRITING READS\n    2 read IDs found (0.02 mill reads processed)\n    2 reads printed to file\n    Generated file: linterrogens.fasta\nPROGRAM END TIME: 04-16-2023 18:40:00\n</code></pre> <pre><code>!cat leptospira.fasta\n</code></pre> <p>If you wished you could go and take these reads and BLAST them over at NCBI-BLAST. There is also the <code>nr</code> BLAST database available on CLIMB-BIG-DATA if you wanted to run <code>blastx</code> on them. The BLAST databases are found in <code>/shared/team/db/blast</code>.</p> <p>It was a bit disappointing that only 33% of the reads in our dataset were assigned. We could try a much bigger database than <code>k2_standard_16gb</code> such as <code>k2_pluspfp</code> which contains protozoal, fungal and plant sequences, as well as taxa contained in <code>k2_standard</code>. </p> <p>To do this we will need to use the Kubernetes cluster in CLIMB-BIG-DATA. With the Kubernetes cluster we can run much bigger tasks requiring much more CPU power, or more memory than in a notebook container like this.</p> <p>The easiest way to get started with Kubernetes is using Nextflow.</p> <p>When you run a Nextflow script it will automagically use Kubernetes.</p> <p>There are a few Kraken2 Nextflow scripts, the simplest one I have found is metashot/kraken2. This will also do a few extra steps like running Bracken.</p> <p>If you wanted to run Kraken2 through Nextflow the same way as before you could run:</p> <pre><code>!nextflow run metashot/kraken2 \\\n  -c /etc/nextflow.config \\\n  --reads canalseq.fastq \\\n  --kraken2_db /shared/public/db/kraken2/k2_standard_16gb \\\n  --read_len 100 \\\n  --outdir canalseq-standard16 \\\n  --single_end\n</code></pre> <p>But - and for our final trick - we would like to use a much bigger database <code>k2_pluspfp</code>. We can just re-check it's size.</p> <pre><code>!du -h -d1 /shared/public/db/kraken2/k2_pluspfp\n</code></pre> <p>As this database is around 145Gb, we can ask Nextflow to give us 200 gigabytes of RAM when running this container, to ensure this database fits easily in memory. We could also ask for a lot more CPUs to speed things along further! Nextflow and Kubernetes will take care of finding a machine the best size for this workflow.</p> <pre><code>!nextflow run metashot/kraken2 \\\n  -c /etc/nextflow.config \\\n  --reads canalseq.fasta \\\n  --kraken2_db /shared/public/db/kraken2/k2_pluspfp \\\n  --read_len 100 \\\n  --outdir canalseq-pluspfp \\\n  --single_end \\\n  --max_memory 200.G \\\n  --max_cpus 64\n</code></pre> <p>Ah, OK this gives 80% assignment! We can go and take a look at this in Krona again.</p> <pre><code>!ktImportTaxonomy -t 5 -m 3 -o krona-canalseq-pluspfp.html canalseq-pluspfp/kraken2/canalseq.kraken2.report\n</code></pre> <pre><code>   [ WARNING ]  Score column already in use; not reading scores.\nLoading taxonomy...\nImporting canalseq-pluspfp/kraken2/canalseq.kraken2.report...\n   [ WARNING ]  The following taxonomy IDs were not found in the local\n                database and were set to root (if they were recently added to\n                NCBI, use updateTaxonomy.sh to update the local database):\n                42857\nWriting krona-canalseq-pluspfp.html...\n</code></pre> <p>Lots more taxa to explore if you open up <code>krona-canalseq-pluspfp.htm</code> in the browser on the left!</p>"},{"location":"walkthroughs/metagenomics-tutorial/#r-and-rstudio","title":"R and RStudio","text":"<p>Let's finish up by comparing the results of Kraken2 using the standard-16 database versus the full-fat PlusPFP database!</p> <p>We can use a nice little tool called <code>taxpasta</code> to take the results of the two Kraken2 runs, merge them together and write out a tabular format file that will load easily into R.</p> <pre><code>!cp canalseq-pluspfp/kraken2/canalseq.kraken2.report pluspfp.report\n!cp canalseq-standard16/kraken2/canalseq.kraken2.report standard16.report\n!taxpasta merge --profiler kraken2  --output-format tsv --add-name --add-rank --taxonomy /shared/public/db/taxonomy -o canalseq.merged.tsv pluspfp.report standard16.report\n</code></pre> <p>Now we can do some magic with R.</p> <p>For the next part, either change the running kernel type (using the dropdown menu on the top right) to \"R\", or flip over to RStudio (via File - New Launcher in the menu bar).</p> <pre><code>library(tidyverse)\n</code></pre> <pre><code>UsageError: Cell magic `%%R` not found.\n</code></pre> <pre><code>df = read_tsv(\"canalseq.merged.tsv\", show_col_types=F)\n</code></pre> <pre><code>head(df)\n</code></pre> A tibble: 6 \u00d7 5 taxonomy_idnamerankpluspfpstandard16 &lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;      0NA                NA          775124921      1root              no rank       45   20 131567cellular organismsno rank     2167  381   2759Eukaryota         superkingdom 742    1  33090Viridiplantae     kingdom       12    0  35493Streptophyta      phylum         0    0 <p>A bit of <code>tidyverse</code> magic can plot us the top 20 genera for each of the Kraken2 databases used side by side.</p> <pre><code>df %&gt;% \n    gather(key=\"db\", val=\"count\", 4:5) %&gt;%\n    filter(rank=='genus') %&gt;%\n    slice_max(n=20, count, by=\"db\") %&gt;%\n    ggplot(., aes(x=name, y=count, fill=db)) +\n    geom_bar(stat='identity', position=\"dodge\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n</code></pre> <p></p> <p>And that's the end of the tutorial. </p>"}]}